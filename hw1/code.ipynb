{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Домашнее задание 1. Извлечение ключевых слов\n\nПри выполнении домашнего задания можно пользоваться материалами лекций и семинаров.\n\n### Описание задания\n\n1. (1 балл) Подготовить мини-корпус (не меньше 4 текстов, примерный общий объём - 3-5 тысяч токенов) с разметкой ключевых слов. \nПредполагается, что вы найдете источник текстов, в котором **уже выделены** ключевые слова.\nУкажите источник корпуса и опишите, в каком виде там были представлены ключевые слова.\n\n2. (2 балла) Разметить ключевые слова самостоятельно. Оценить пересечение с имеющейся разметкой.\nСоставить эталон разметки (например, пересечение или объединение вашей разметки и исходной).\n\n3. (2 балла) Применить к этому корпусу 3 метода извлечения ключевых слов на выбор (RAKE, TextRank, tf*idf, OKAPI BM25, ...)\n\n4. (2 балла) Составить морфологические/синтаксические шаблоны для ключевых слов и фраз, выделить соответствующие им подстроки из корпуса (например, именные группы Adj+Noun).\nПрименить эти фильтры к спискам ключевых слов.\n\n4. (2  балла) Оценить точность, полноту, F-меру выбранных методов относительно эталона:\nс учётом морфосинтаксических шаблонов и без них.\n\n5. (1 балл) Описать ошибки автоматического выделения ключевых слов (что выделяется лишнее, что не выделяется);\nпредложить свои методы решения этих проблем.","metadata":{}},{"cell_type":"code","source":"# !pip install keybert\n# !pip install pymorphy2\n# !pip install pymorphy2-dicts-uk","metadata":{"execution":{"iopub.status.busy":"2022-11-19T07:34:27.816578Z","iopub.execute_input":"2022-11-19T07:34:27.817520Z","iopub.status.idle":"2022-11-19T07:35:06.169539Z","shell.execute_reply.started":"2022-11-19T07:34:27.817482Z","shell.execute_reply":"2022-11-19T07:35:06.168355Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting keybert\n  Downloading keybert-0.7.0.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting sentence-transformers>=0.3.8\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m708.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.2 in /opt/conda/lib/python3.7/site-packages (from keybert) (1.0.2)\nRequirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.7/site-packages (from keybert) (1.21.6)\nRequirement already satisfied: rich>=10.4.0 in /opt/conda/lib/python3.7/site-packages (from keybert) (12.6.0)\nRequirement already satisfied: commonmark<0.10.0,>=0.9.0 in /opt/conda/lib/python3.7/site-packages (from rich>=10.4.0->keybert) (0.9.1)\nRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from rich>=10.4.0->keybert) (2.12.0)\nRequirement already satisfied: typing-extensions<5.0,>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from rich>=10.4.0->keybert) (4.1.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.22.2->keybert) (3.1.0)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.22.2->keybert) (1.7.3)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.22.2->keybert) (1.0.1)\nRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers>=0.3.8->keybert) (4.20.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence-transformers>=0.3.8->keybert) (4.64.0)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers>=0.3.8->keybert) (1.11.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from sentence-transformers>=0.3.8->keybert) (0.12.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from sentence-transformers>=0.3.8->keybert) (3.7)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from sentence-transformers>=0.3.8->keybert) (0.1.97)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers>=0.3.8->keybert) (0.10.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (4.13.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.7.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.28.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2021.11.10)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (0.12.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers>=0.3.8->keybert) (8.0.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (9.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.8.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2022.9.24)\nBuilding wheels for collected packages: keybert, sentence-transformers\n  Building wheel for keybert (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keybert: filename=keybert-0.7.0-py3-none-any.whl size=23799 sha256=0007d92ed2632161433bea7096acfb516e34133be315186c69a951173815e8af\n  Stored in directory: /root/.cache/pip/wheels/85/0d/12/77d219f3ebbb22dc22234b4d665886c0eace86a26eca0aa72b\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=98a5bcf9ce2fb33569f2b76171280b0b70d91118363ef8641a83f8317f3b97bc\n  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\nSuccessfully built keybert sentence-transformers\nInstalling collected packages: sentence-transformers, keybert\nSuccessfully installed keybert-0.7.0 sentence-transformers-2.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting pymorphy2\n  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m457.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting pymorphy2-dicts-ru<3.0,>=2.4\n  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: docopt>=0.6 in /opt/conda/lib/python3.7/site-packages (from pymorphy2) (0.6.2)\nCollecting dawg-python>=0.7.1\n  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\nInstalling collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\nSuccessfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting pymorphy2-dicts-uk\n  Downloading pymorphy2_dicts_uk-2.4.1.1.1460299261-py2.py3-none-any.whl (5.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n\u001b[?25hInstalling collected packages: pymorphy2-dicts-uk\nSuccessfully installed pymorphy2-dicts-uk-2.4.1.1.1460299261\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from __future__ import annotations\n\nfrom string import punctuation\nfrom tqdm import tqdm\n\nimport pandas as pd\nimport numpy as np\n\nimport spacy\nimport pymorphy2\nfrom keybert import KeyBERT\nfrom nltk.corpus import stopwords\nfrom nltk import tokenize\n\nmorph = pymorphy2.MorphAnalyzer(lang='uk')\nlemm = spacy.load('en_core_web_sm')\nstopwords_eng = stopwords.words(\"english\")\npunctuation += '—…«»'\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:19:10.313301Z","iopub.execute_input":"2022-11-19T09:19:10.313702Z","iopub.status.idle":"2022-11-19T09:19:11.063026Z","shell.execute_reply.started":"2022-11-19T09:19:10.313667Z","shell.execute_reply":"2022-11-19T09:19:11.062011Z"},"trusted":true},"execution_count":149,"outputs":[]},{"cell_type":"markdown","source":"# 1. Создание корпуса\n\n(1 балл) Подготовить мини-корпус (не меньше 4 текстов, примерный общий объём - 3-5 тысяч токенов) с разметкой ключевых слов. Предполагается, что вы найдете источник текстов, в котором уже выделены ключевые слова. Укажите источник корпуса и опишите, в каком виде там были представлены ключевые слова.\n\n**Источник текстов:** [соревнование по Keyword Extraction от FaceBook](https://www.kaggle.com/competitions/facebook-recruiting-iii-keyword-extraction). Из исходного train датасета были взять рандомные 20 текстов, общая сумма токенов в которых составляет 3 148. Ключевые слова помечены как тэги.  ","metadata":{}},{"cell_type":"markdown","source":"## 1.1 Корпус","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/nlp-hw1-dataset/Train_sample.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:19:11.484197Z","iopub.execute_input":"2022-11-19T09:19:11.484789Z","iopub.status.idle":"2022-11-19T09:19:11.504604Z","shell.execute_reply.started":"2022-11-19T09:19:11.484749Z","shell.execute_reply":"2022-11-19T09:19:11.503491Z"},"trusted":true},"execution_count":150,"outputs":[{"execution_count":150,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                                              Title  \\\n0      916626      I dont understand why this log4j.xml is wrong   \n1      768258                clone git repository via active FTP   \n2     2368609  How to print XSLT version supported by Xalan i...   \n3      131806        Hibernate mapping for cyclic relation ships   \n4     4213547                 ffmpeg: how to add cartoon effect?   \n\n                                                Body             Tags  \n0  <p>I wanna log into cassandra db with log4j.</...  log4j cassandra  \n1  <p>I'm a branch office worker and have uploade...    git ftp clone  \n2  <p>I am using Xalan C++ library and I want to ...            xalan  \n3  <p>I am learning a hibernate and I am developi...        hibernate  \n4  <p>I was wondering if anyone out there has use...           ffmpeg  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Title</th>\n      <th>Body</th>\n      <th>Tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>916626</td>\n      <td>I dont understand why this log4j.xml is wrong</td>\n      <td>&lt;p&gt;I wanna log into cassandra db with log4j.&lt;/...</td>\n      <td>log4j cassandra</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>768258</td>\n      <td>clone git repository via active FTP</td>\n      <td>&lt;p&gt;I'm a branch office worker and have uploade...</td>\n      <td>git ftp clone</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2368609</td>\n      <td>How to print XSLT version supported by Xalan i...</td>\n      <td>&lt;p&gt;I am using Xalan C++ library and I want to ...</td>\n      <td>xalan</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>131806</td>\n      <td>Hibernate mapping for cyclic relation ships</td>\n      <td>&lt;p&gt;I am learning a hibernate and I am developi...</td>\n      <td>hibernate</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4213547</td>\n      <td>ffmpeg: how to add cartoon effect?</td>\n      <td>&lt;p&gt;I was wondering if anyone out there has use...</td>\n      <td>ffmpeg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print('Сумма токенов:', df['Body'].apply(lambda x: len(x.split())).sum())","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:19:12.017861Z","iopub.execute_input":"2022-11-19T09:19:12.018233Z","iopub.status.idle":"2022-11-19T09:19:12.025871Z","shell.execute_reply.started":"2022-11-19T09:19:12.018201Z","shell.execute_reply":"2022-11-19T09:19:12.024717Z"},"trusted":true},"execution_count":151,"outputs":[{"name":"stdout","text":"Сумма токенов: 3148\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 1.2 Preprocessing","metadata":{}},{"cell_type":"code","source":"def remove_punctuation(text: str) -> str:\n    for i in punctuation:\n        text = text.replace(i, ' ')\n    return text\n\n\ndef lemmatization(text: str) -> list:\n    return [token.lemma_ for token in lemm(text)]\n\n\ndef remove_stopwords(text: list) -> str:\n    clean_text = [word for word in text if (word not in stopwords_eng) and (word != ' ')]\n    return ' '.join(clean_text)\n\n\ndef preprocessing(text: str):\n    lemm_text = remove_punctuation(text)\n    lemm_text = lemmatization(lemm_text)\n    lemm_text = remove_stopwords(lemm_text)\n    return lemm_text","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:19:12.999577Z","iopub.execute_input":"2022-11-19T09:19:13.000600Z","iopub.status.idle":"2022-11-19T09:19:13.008266Z","shell.execute_reply.started":"2022-11-19T09:19:13.000553Z","shell.execute_reply":"2022-11-19T09:19:13.007145Z"},"trusted":true},"execution_count":152,"outputs":[]},{"cell_type":"code","source":"df['Body'] = df['Body'].apply(preprocessing)","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:19:13.411363Z","iopub.execute_input":"2022-11-19T09:19:13.412095Z","iopub.status.idle":"2022-11-19T09:19:14.343167Z","shell.execute_reply.started":"2022-11-19T09:19:13.412052Z","shell.execute_reply":"2022-11-19T09:19:14.342143Z"},"trusted":true},"execution_count":153,"outputs":[]},{"cell_type":"markdown","source":"# 2. Cамостоятельная разметка ключевых слов\n\n(2 балла) Разметить ключевые слова самостоятельно. Оценить пересечение с имеющейся разметкой. Составить эталон разметки (например, пересечение или объединение вашей разметки и исходной).","metadata":{}},{"cell_type":"code","source":"fb_tags = [i.split() for i in df['Tags']]\nmy_tags = [\n    ['log4j', 'cassandra'], \n    ['ftp', 'server', 'git', 'clone', 'password'], \n    ['xalan', 'c++', 'xsl'], \n    ['hibernate', 'dependency'], \n    ['frie0r', 'ffmpeg'], \n    ['php', 'javascript', 'jquery', 'ajax', 'fwrite'], \n    ['github', 'bespin'], \n    ['bzip2', 'EOF'], \n    ['javadocs'], \n    ['zip'], \n    ['software',  'ghost'], \n    ['transactions', 'R'], \n    ['hdb2ddl', 'hibernate'], \n    ['ajax', 'php', 'mysql', 'post'], \n    ['varchar', 'bigint'], \n    ['php', 'counter'], \n    ['xml', 'flash'], \n    ['language', 'search'], \n    ['array'], \n    ['location', 'android', 'map', 'API']\n]","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:19:14.344986Z","iopub.execute_input":"2022-11-19T09:19:14.345447Z","iopub.status.idle":"2022-11-19T09:19:14.355151Z","shell.execute_reply.started":"2022-11-19T09:19:14.345401Z","shell.execute_reply":"2022-11-19T09:19:14.352624Z"},"trusted":true},"execution_count":154,"outputs":[]},{"cell_type":"code","source":"intercept = 0\nfor tag1, tag2 in zip(fb_tags, my_tags):\n    if set(tag1) == set(tag2):\n        intercept += 1\nprint(f'Разметка совпадает в {intercept} строках из {len(my_tags)} строк.')","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:19:14.648321Z","iopub.execute_input":"2022-11-19T09:19:14.648737Z","iopub.status.idle":"2022-11-19T09:19:14.655462Z","shell.execute_reply.started":"2022-11-19T09:19:14.648705Z","shell.execute_reply":"2022-11-19T09:19:14.654348Z"},"trusted":true},"execution_count":155,"outputs":[{"name":"stdout","text":"Разметка совпадает в 5 строках из 20 строк.\n","output_type":"stream"}]},{"cell_type":"code","source":"gold_tags = [list(set(tag1) | set(tag2)) for tag1, tag2 in zip(fb_tags, my_tags)]\ngold_tags","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:19:15.111216Z","iopub.execute_input":"2022-11-19T09:19:15.112301Z","iopub.status.idle":"2022-11-19T09:19:15.122929Z","shell.execute_reply.started":"2022-11-19T09:19:15.112248Z","shell.execute_reply":"2022-11-19T09:19:15.121597Z"},"trusted":true},"execution_count":156,"outputs":[{"execution_count":156,"output_type":"execute_result","data":{"text/plain":"[['cassandra', 'log4j'],\n ['password', 'server', 'clone', 'git', 'ftp'],\n ['c++', 'xsl', 'xalan'],\n ['hibernate', 'dependency'],\n ['frie0r', 'ffmpeg'],\n ['ajax', 'fwrite', 'jquery', 'javascript', 'php'],\n ['github', 'bespin'],\n ['EOF', 'bzip2'],\n ['java', 'javadocs', 'javadoc'],\n ['zip'],\n ['ghost', 'software'],\n ['r', 'transactions', 'R'],\n ['hdb2ddl', 'java', 'hibernate'],\n ['ajax', 'mysql', 'php', 'post'],\n ['bigint', 'varchar', 'sql-server-2005'],\n ['counter', 'php'],\n ['flash', 'xml'],\n ['language', 'search', 'sql'],\n ['arrays', 'c', 'array'],\n ['map', 'location', 'API', 'android']]"},"metadata":{}}]},{"cell_type":"markdown","source":"# 3. Извлечение ключевых слов\n\n(2 балла) Применить к этому корпусу 3 метода извлечения ключевых слов: RAKE, TextRank, tf*idf, OKAPI BM25","metadata":{}},{"cell_type":"code","source":"n = 5","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:19:16.368218Z","iopub.execute_input":"2022-11-19T09:19:16.368616Z","iopub.status.idle":"2022-11-19T09:19:16.376822Z","shell.execute_reply.started":"2022-11-19T09:19:16.368582Z","shell.execute_reply":"2022-11-19T09:19:16.375793Z"},"trusted":true},"execution_count":157,"outputs":[]},{"cell_type":"markdown","source":"# 3.1 TF*IDF","metadata":{}},{"cell_type":"code","source":"def calculate_tfidf(texts: list) -> list[list[str]]:\n    tfidf_vectorizer = TfidfVectorizer(use_idf=True, norm='l2')\n    matrix = tfidf_vectorizer.fit_transform(texts).toarray()\n    vocabulary = tfidf_vectorizer.get_feature_names_out()\n    \n    tags, values = [], []\n    index2token = {index: token for token, index in (tfidf_vectorizer.vocabulary_).items()}\n    for row in matrix:\n        max_args = [arg for arg in (-row).argsort()[:n]]\n        max_tokens = [index2token[arg] for arg in max_args]\n        max_values = np.take(row, max_args)\n        tags.append(max_tokens)\n        values.append(max_values)\n    return tags, values","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:19:19.131687Z","iopub.execute_input":"2022-11-19T09:19:19.132047Z","iopub.status.idle":"2022-11-19T09:19:19.140509Z","shell.execute_reply.started":"2022-11-19T09:19:19.132015Z","shell.execute_reply":"2022-11-19T09:19:19.139334Z"},"trusted":true},"execution_count":158,"outputs":[]},{"cell_type":"code","source":"tags_tfidf, values_tfidf = calculate_tfidf(df['Body'])\ntags_tfidf","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:19:19.509319Z","iopub.execute_input":"2022-11-19T09:19:19.509708Z","iopub.status.idle":"2022-11-19T09:19:19.527446Z","shell.execute_reply.started":"2022-11-19T09:19:19.509673Z","shell.execute_reply":"2022-11-19T09:19:19.526472Z"},"trusted":true},"execution_count":159,"outputs":[{"execution_count":159,"output_type":"execute_result","data":{"text/plain":"[['log4j', 'gt', 'lt', 'name', 'cassandra'],\n ['ftp', 'git', 'company', 'repo', 'com'],\n ['xalan', 'support', 'version', 'xsl', 'print'],\n ['address', 'person', 'private', 'class', 'model'],\n ['cartoon', 'output', 'look', 'openmovieeditor', 'filtereffect'],\n ['gt', 'lt', 'type', 'script', 'value'],\n ['bespin', 'https', 'mozillalab', 'mozilla', 'vcsintegration'],\n ['bzip2', 'limit', 'block', 'bzip2recover', 'eof'],\n ['search', 'id', 'tag', 'javadoc', 'frame'],\n ['zip', 'directory', 'p1', 'unzip', 'subdirectory'],\n ['software', 'backup', 'imaging', 'question', 'http'],\n ['code', 'transaction', 'apriori', '2322', 'algorithm'],\n ['gt', 'lt', 'public', 'classeb', 'code'],\n ['completely', 'post', 'file', 'finish', 'ajax'],\n ['datatype', 'phone', 'store', '2005', 'sql'],\n ['colcount', 'mysql', 'result', 'artwork', 'row'],\n ['url', 'urlxml', 'trace', 'xml', 'success'],\n ['etc', 'language', 'establish', 'side', 'category'],\n ['code', 'value', 'array', 'em', 'element'],\n ['location', 'android', 'import', 'gms', 'map']]"},"metadata":{}}]},{"cell_type":"markdown","source":"## 3.2 OKAPI BM25","metadata":{}},{"cell_type":"code","source":"def calculate_bm25(texts: list, k: int = 2, b: int = 0.75) -> list[list[str]]:\n    # tf\n    count_vectorizer = CountVectorizer()\n    count = count_vectorizer.fit_transform(texts).toarray()\n    tf = count\n\n    # idf\n    tfidf_vectorizer = TfidfVectorizer(use_idf=True, norm='l2')\n    tfidf = tfidf_vectorizer.fit_transform(texts).toarray()\n    vocabulary = tfidf_vectorizer.get_feature_names_out()\n    idf = tfidf_vectorizer.idf_\n    idf = np.expand_dims(idf, axis=0)\n\n    # расчет количества слов в каждом документе - l(d)\n    len_d = tf.sum(axis=1)\n    \n    # расчет среднего количества слов документов корпуса - avdl\n    avdl = len_d.mean()\n\n    # расчет числителя\n    A = idf * tf * (k + 1)\n\n    # расчет знаменателя\n    B_1 = (k * (1 - b + b * len_d / avdl))\n    B_1 = np.expand_dims(B_1, axis=-1)\n    B = tf + B_1\n\n    # BM25\n    matrix = A / B\n    \n    # get tags\n    index2token = {index: token for token, index in (count_vectorizer.vocabulary_).items()}\n    tags = []\n    values = []\n    for row in matrix:\n        max_args = [arg for arg in (-row).argsort()[:n]]\n        max_tokens = [index2token[arg] for arg in max_args]\n        max_values = np.take(row, max_args)\n        tags.append(max_tokens)\n        values.append(max_values)\n    return tags, values","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:19:20.624228Z","iopub.execute_input":"2022-11-19T09:19:20.624625Z","iopub.status.idle":"2022-11-19T09:19:20.636510Z","shell.execute_reply.started":"2022-11-19T09:19:20.624590Z","shell.execute_reply":"2022-11-19T09:19:20.635336Z"},"trusted":true},"execution_count":160,"outputs":[]},{"cell_type":"code","source":"tags_bm25, values_bm25 = calculate_bm25(df['Body'])\ntags_bm25","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:19:21.198277Z","iopub.execute_input":"2022-11-19T09:19:21.198680Z","iopub.status.idle":"2022-11-19T09:19:21.220402Z","shell.execute_reply.started":"2022-11-19T09:19:21.198646Z","shell.execute_reply":"2022-11-19T09:19:21.219495Z"},"trusted":true},"execution_count":161,"outputs":[{"execution_count":161,"output_type":"execute_result","data":{"text/plain":"[['log4j', 'appender', 'cassandra', 'param', 'jdbc'],\n ['ftp', 'repo', 'company', 'git', 'username'],\n ['xalan', 'xsl', 'print', 'support', 'version'],\n ['address', 'person', 'dependency', 'private', 'model'],\n ['cartoon', 'openmovieeditor', 'ffmpeg', 'video', 'filtereffect'],\n ['script', 'text', 'type', 'gt', 'value'],\n ['bespin', 'https', 'mozillalab', 'mozilla', 'vcsintegration'],\n ['bzip2', 'limit', 'block', 'bzip2recover', 'eof'],\n ['id', 'tag', 'javadoc', 'search', 'frame'],\n ['zip', 'directory', 'ignore', '7z', 'subdirectory'],\n ['software', 'backup', 'imaging', 'installation', '2010'],\n ['transaction', '2322', 'apriori', 'algorithm', '1141'],\n ['classeb', 'integer', 'cb', 'annotazione', 'numero'],\n ['finish', 'completely', 'post', 'ajax', 'run'],\n ['datatype', 'phone', 'store', '2005', 'sql'],\n ['colcount', 'artwork', 'mysql', 'result', 'cell'],\n ['urlxml', 'trace', 'url', 'success', 'flash'],\n ['language', 'establish', 'side', 'category', 'define'],\n ['array', 'em', 'value', '1024', 'iteration'],\n ['android', 'import', 'location', 'gms', 'loc']]"},"metadata":{}}]},{"cell_type":"markdown","source":"## 3.3 Keybert","metadata":{}},{"cell_type":"code","source":"kw_model = KeyBERT('clips/mfaq')","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:19:24.761440Z","iopub.execute_input":"2022-11-19T09:19:24.762411Z","iopub.status.idle":"2022-11-19T09:19:28.726778Z","shell.execute_reply.started":"2022-11-19T09:19:24.762372Z","shell.execute_reply":"2022-11-19T09:19:28.725783Z"},"trusted":true},"execution_count":162,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/configuration_utils.py:364: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n","output_type":"stream"}]},{"cell_type":"code","source":"def calculate_kbert(texts: list) -> list[list[str]]:\n    tags = []\n    values = []\n    texts_kw = kw_model.extract_keywords(texts)\n    for text_kw in texts_kw:\n        keywords = [word for word, prob in text_kw]\n        prob = [prob for word, prob in text_kw]\n        tags.append(keywords)\n        values.append(prob)\n    return tags, values","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:19:28.728678Z","iopub.execute_input":"2022-11-19T09:19:28.729156Z","iopub.status.idle":"2022-11-19T09:19:28.735740Z","shell.execute_reply.started":"2022-11-19T09:19:28.729118Z","shell.execute_reply":"2022-11-19T09:19:28.734755Z"},"trusted":true},"execution_count":163,"outputs":[]},{"cell_type":"code","source":"tags_kbert, values_kbert = calculate_kbert(df['Body'])\ntags_kbert","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:19:28.737294Z","iopub.execute_input":"2022-11-19T09:19:28.737951Z","iopub.status.idle":"2022-11-19T09:19:29.502464Z","shell.execute_reply.started":"2022-11-19T09:19:28.737915Z","shell.execute_reply":"2022-11-19T09:19:29.501256Z"},"trusted":true},"execution_count":164,"outputs":[{"execution_count":164,"output_type":"execute_result","data":{"text/plain":"[['logg', 'configuration', 'cql', 'log', 'logging'],\n ['ftp', 'server', 'install', 'microsoft', 'command'],\n ['support', 'xsl', 'version', 'want', 'xslt'],\n ['learn', 'project', 'class', 'mapping', 'develop'],\n ['exe', 'ffmpeg', 'openmovieeditor', 'video', 'cartoon'],\n ['execute', 'config', 'function', 'xmlhttprequest', 'xhtml'],\n ['github', 'git', 'vcsintegration', 'mozillalab', 'wiki'],\n ['error', 'modify', 'problem', 'eof', 'bzip2'],\n ['code', 'search', 'documentation', 'add', 'tag'],\n ['unzip', 'tool', 'code', 'command', 'file'],\n ['backup', 'blockquote', 'software', 'like', 'disk'],\n ['code', 'read', 'try', 'function', 'tr'],\n ['string', 'setnumero', 'setcb', 'encode', 'cb'],\n ['php', 'task', 'user', 'post', 'download'],\n ['sql', 'blockquote', 'datatype', 'href', 'example'],\n ['syntax', 'mysql', 'add', 'code', 'php'],\n ['xmlurl', 'urlxml', 'xml', 'figure', 'stackoverflow'],\n ['sql', 'logical', 'define', 'definition', 'googles'],\n ['array', 'initialize', 'element', 'build', 'solve'],\n ['googlemap', 'maps', 'getmap', 'bitmapdescriptorfactory', 'addmarker']]"},"metadata":{}}]},{"cell_type":"markdown","source":"# 4. Составить шаблоны\n\n(2 балла) Составить морфологические/синтаксические шаблоны для ключевых слов и фраз, выделить соответствующие им подстроки из корпуса (например, именные группы Adj+Noun). Применить эти фильтры к спискам ключевых слов.","metadata":{}},{"cell_type":"code","source":"def text_to_tokens(text: str) -> list[list[str]]:\n    return [tokenize.word_tokenize(sentence) for sentence in tokenize.sent_tokenize(text)]\n\n\ndef tokens_to_pos(tokens: list[list[str]]) -> list[list[str]]:\n    poses = []\n    for sentence in tokens:\n        sent_poses = []\n        for token in sentence:\n            sent_poses.append(lemm(token)[0].pos_)\n        poses.append(sent_poses)\n    return poses\n\n    \ndef extract_template(text: str, first_pos: str = \"ADJ\", second_pos: str = \"NOUN\") -> str:\n    tokens = text_to_tokens(text)\n    poses = tokens_to_pos(tokens)\n    templates = []\n    for i in range(len(poses)):\n        sent_tokens = tokens[i]\n        sent_pos = poses[i]\n        sent_templates = []\n        for j in range(len(sent_pos) - 1):\n            if sent_pos[j] == first_pos and sent_pos[j+1] == second_pos:\n                sent_templates.append(sent_tokens[j])\n                sent_templates.append(sent_tokens[j+1])\n        templates.append(sent_templates)\n    return \".\".join([\" \".join(sent) for sent in templates])","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:19:29.505016Z","iopub.execute_input":"2022-11-19T09:19:29.505565Z","iopub.status.idle":"2022-11-19T09:19:29.521888Z","shell.execute_reply.started":"2022-11-19T09:19:29.505510Z","shell.execute_reply":"2022-11-19T09:19:29.520628Z"},"trusted":true},"execution_count":165,"outputs":[]},{"cell_type":"code","source":"df['Adj+Noun'] = df['Body'].apply(lambda x: extract_template(x, first_pos='ADJ', second_pos='NOUN'))\ndf['Noun+Noun'] = df['Body'].apply(lambda x: extract_template(x, first_pos='NOUN', second_pos='NOUN'))","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:19:29.523233Z","iopub.execute_input":"2022-11-19T09:19:29.524262Z","iopub.status.idle":"2022-11-19T09:20:02.460360Z","shell.execute_reply.started":"2022-11-19T09:19:29.524227Z","shell.execute_reply":"2022-11-19T09:20:02.459372Z"},"trusted":true},"execution_count":166,"outputs":[]},{"cell_type":"code","source":"def get_best_tags(tags_NN, values_NN, tags_AN, values_AN) -> list[list[str]]:\n    best_tokens = []\n    for t_nn, t_an, v_nn, v_an in zip(tags_NN, tags_AN, \n                                      values_NN, values_AN):\n        concat_v = np.concatenate((v_nn, v_an), axis=0)\n        concat_t = t_nn + t_an\n        max_args = [arg for arg in (-concat_v).argsort()[:n]]\n        best_tokens.append(list(np.take(concat_t, max_args)))\n    return best_tokens","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:20:02.461841Z","iopub.execute_input":"2022-11-19T09:20:02.462437Z","iopub.status.idle":"2022-11-19T09:20:02.469468Z","shell.execute_reply.started":"2022-11-19T09:20:02.462397Z","shell.execute_reply":"2022-11-19T09:20:02.468545Z"},"trusted":true},"execution_count":167,"outputs":[]},{"cell_type":"code","source":"# TF-IDF\ntags_tfidf_NN, values_tfidf_NN = calculate_tfidf(df['Noun+Noun'])\ntags_tfidf_AN, values_tfidf_AN = calculate_tfidf(df['Adj+Noun'])\ntags_tfidf_templ = get_best_tags(tags_tfidf_NN, values_tfidf_NN, tags_tfidf_AN, values_tfidf_AN)\n\n# BM25\ntags_bm25_NN, values_bm25_NN = calculate_bm25(df['Noun+Noun'])\ntags_bm25_AN, values_bm25_AN = calculate_bm25(df['Adj+Noun'])\ntags_bm25_templ = get_best_tags(tags_bm25_NN, values_bm25_NN, tags_bm25_AN, values_bm25_AN)\n\n# KeyBERT\ntags_kbert_NN, values_kbert_NN = calculate_kbert(df['Noun+Noun'])\ntags_kbert_AN, values_kbert_AN = calculate_kbert(df['Adj+Noun'])\ntags_kbert_templ = get_best_tags(tags_kbert_NN, values_kbert_NN, tags_kbert_AN, values_kbert_AN)","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:20:02.471041Z","iopub.execute_input":"2022-11-19T09:20:02.471687Z","iopub.status.idle":"2022-11-19T09:20:02.882993Z","shell.execute_reply.started":"2022-11-19T09:20:02.471650Z","shell.execute_reply":"2022-11-19T09:20:02.881550Z"},"trusted":true},"execution_count":168,"outputs":[]},{"cell_type":"markdown","source":"# 5. Точность, полнота, F-мера\n\n(2 балла) Оценить точность, полноту, F-меру выбранных методов относительно эталона: с учётом морфосинтаксических шаблонов и без них.","metadata":{}},{"cell_type":"markdown","source":"$$ Precision =  \\frac{|correctly\\;extracted\\;entities|}  {|entities\\;extracted\\;by\\;model|} $$\n\n\n$$ Recall =  \\frac{|correctly\\;extracted\\;entities|}  {|entities\\;that\\;are\\;keywords|} $$\n\n\n$$ F1 =  \\frac{2*Precision*Recall}  {Precision + Recall} $$","metadata":{}},{"cell_type":"code","source":"def precision(pred_entities: list, gold_entities: list) -> float:\n    corr = 0\n    all_ent = 0\n    \n    for pred, gold in zip(pred_entities, gold_entities):\n        corr += (len(set(pred) & set(gold)))\n        all_ent += len(pred)\n    return round(corr / all_ent, 4)\n\n        \ndef recall(pred_entities: list, gold_entities: list) -> float:\n    corr = 0\n    all_ent = 0\n    \n    for pred, gold in zip(pred_entities, gold_entities):\n        corr += (len(set(pred) & set(gold)))\n        all_ent += len(gold)\n    return round(corr / all_ent, 4)\n\n\ndef f1(pred: list, gold: list) -> float:\n    p = precision(pred, gold)\n    r = recall(pred, gold)\n    return round((2 * p * r) / (p + r), 4)","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:20:02.892175Z","iopub.execute_input":"2022-11-19T09:20:02.895556Z","iopub.status.idle":"2022-11-19T09:20:02.911837Z","shell.execute_reply.started":"2022-11-19T09:20:02.895484Z","shell.execute_reply":"2022-11-19T09:20:02.910615Z"},"trusted":true},"execution_count":169,"outputs":[]},{"cell_type":"markdown","source":"## 5.1 Без учета шаблонов\n### 5.1.1 TF-IDF","metadata":{}},{"cell_type":"code","source":"print('Precision:', precision(tags_tfidf, gold_tags))\nprint('Recall:', recall(tags_tfidf, gold_tags))\nprint('F1:', f1(tags_tfidf, gold_tags))","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:20:02.917643Z","iopub.execute_input":"2022-11-19T09:20:02.920775Z","iopub.status.idle":"2022-11-19T09:20:02.933932Z","shell.execute_reply.started":"2022-11-19T09:20:02.920721Z","shell.execute_reply":"2022-11-19T09:20:02.932734Z"},"trusted":true},"execution_count":170,"outputs":[{"name":"stdout","text":"Precision: 0.19\nRecall: 0.3393\nF1: 0.2436\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 5.1.2 BM25","metadata":{}},{"cell_type":"code","source":"print('Precision:', precision(tags_bm25, gold_tags))\nprint('Recall:', recall(tags_bm25, gold_tags))\nprint('F1:', f1(tags_bm25, gold_tags))","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:20:02.939580Z","iopub.execute_input":"2022-11-19T09:20:02.942715Z","iopub.status.idle":"2022-11-19T09:20:02.953492Z","shell.execute_reply.started":"2022-11-19T09:20:02.942664Z","shell.execute_reply":"2022-11-19T09:20:02.952192Z"},"trusted":true},"execution_count":171,"outputs":[{"name":"stdout","text":"Precision: 0.2\nRecall: 0.3571\nF1: 0.2564\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 5.1.3 KeyBERT","metadata":{}},{"cell_type":"code","source":"print('Precision:', precision(tags_kbert, gold_tags))\nprint('Recall:', recall(tags_kbert, gold_tags))\nprint('F1:', f1(tags_kbert, gold_tags))","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:20:02.959450Z","iopub.execute_input":"2022-11-19T09:20:02.960414Z","iopub.status.idle":"2022-11-19T09:20:02.973960Z","shell.execute_reply.started":"2022-11-19T09:20:02.960363Z","shell.execute_reply":"2022-11-19T09:20:02.972595Z"},"trusted":true},"execution_count":172,"outputs":[{"name":"stdout","text":"Precision: 0.13\nRecall: 0.2321\nF1: 0.1667\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 5.2 С учетом шаблонов\n### 5.2.1 TF-IDF","metadata":{}},{"cell_type":"code","source":"print('Precision:', precision(tags_tfidf_templ, gold_tags))\nprint('Recall:', recall(tags_tfidf_templ, gold_tags))\nprint('F1:', f1(tags_tfidf_templ, gold_tags))","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:20:02.975235Z","iopub.execute_input":"2022-11-19T09:20:02.975844Z","iopub.status.idle":"2022-11-19T09:20:02.990951Z","shell.execute_reply.started":"2022-11-19T09:20:02.975767Z","shell.execute_reply":"2022-11-19T09:20:02.989767Z"},"trusted":true},"execution_count":173,"outputs":[{"name":"stdout","text":"Precision: 0.1\nRecall: 0.1786\nF1: 0.1282\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 5.2.2 BM25","metadata":{}},{"cell_type":"code","source":"print('Precision:', precision(tags_bm25_templ, gold_tags))\nprint('Recall:', recall(tags_bm25_templ, gold_tags))\nprint('F1:', f1(tags_bm25_templ, gold_tags))","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:20:02.992172Z","iopub.execute_input":"2022-11-19T09:20:02.992605Z","iopub.status.idle":"2022-11-19T09:20:03.007823Z","shell.execute_reply.started":"2022-11-19T09:20:02.992561Z","shell.execute_reply":"2022-11-19T09:20:03.006602Z"},"trusted":true},"execution_count":174,"outputs":[{"name":"stdout","text":"Precision: 0.1\nRecall: 0.1786\nF1: 0.1282\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 5.2.3 KeyBERT","metadata":{}},{"cell_type":"code","source":"print('Precision:', precision(tags_kbert_templ, gold_tags))\nprint('Recall:', recall(tags_kbert_templ, gold_tags))\nprint('F1:', f1(tags_kbert_templ, gold_tags))","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:20:03.009904Z","iopub.execute_input":"2022-11-19T09:20:03.010330Z","iopub.status.idle":"2022-11-19T09:20:03.017285Z","shell.execute_reply.started":"2022-11-19T09:20:03.010289Z","shell.execute_reply":"2022-11-19T09:20:03.016029Z"},"trusted":true},"execution_count":175,"outputs":[{"name":"stdout","text":"Precision: 0.1122\nRecall: 0.1964\nF1: 0.1428\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 6. Ошибки автоматического выделения ключевых слов\n\n(1 балл) Описать ошибки автоматического выделения ключевых слов (что выделяется лишнее, что не выделяется); предложить свои методы решения этих проблем.","metadata":{}},{"cell_type":"markdown","source":"Я выбрала достаточно специфичный датасет, содержащий вопросы из StackOverflow. В вопросах часто содержатся не только вопрос и описание проблемы, но и куски кода, что заметно влияет на работу KeyWord Extraction моделей. \n\nНапример, **tf-idf** и **bm25** часто выделяют как ключевые слова _print, import_  \n\nС одной стороны, может иметь смысл на шаге предобработки удалять куски с кодом, потому что, по интуиции, ключевые слова содержатся с вопросе. С друго стороны, и в коде могут содержаться ключевые слова, например импортируемая бибилиотека или модель. \n\nКроме того, модели часто выделяют цифры как ключевые слова (в основном это год или случайный набор цифр). Решение: на этапе предобработки удалять цифры, которые не входят в состав слов (например, оставляем _2_ в _pymorphy2_, но удаляем _2022_).","metadata":{}},{"cell_type":"code","source":"def false_positive(pred_tags: list, gold_tags: list, model: str) ->list:\n    print(f'Keywords ошибочно предложенные моделью {model}\\n')\n    fp = [set(pred) - set(gold) for pred, gold in zip(pred_tags, gold_tags)]\n    return fp\n\n\ndef false_negative(pred_tags: list, gold_tags: list, model: str) ->list:\n    print(f'Keywords ошибочно НЕ предложенные моделью {model}\\n')\n    fn = [set(gold) - set(pred) for pred, gold in zip(pred_tags, gold_tags)]\n    return fn","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:20:03.019017Z","iopub.execute_input":"2022-11-19T09:20:03.019880Z","iopub.status.idle":"2022-11-19T09:20:03.027666Z","shell.execute_reply.started":"2022-11-19T09:20:03.019846Z","shell.execute_reply":"2022-11-19T09:20:03.026594Z"},"trusted":true},"execution_count":176,"outputs":[]},{"cell_type":"code","source":"false_positive(tags_tfidf, gold_tags, model='tf-idf')","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:20:03.029322Z","iopub.execute_input":"2022-11-19T09:20:03.030021Z","iopub.status.idle":"2022-11-19T09:20:03.043049Z","shell.execute_reply.started":"2022-11-19T09:20:03.029987Z","shell.execute_reply":"2022-11-19T09:20:03.041975Z"},"trusted":true},"execution_count":177,"outputs":[{"name":"stdout","text":"Keywords ошибочно предложенные моделью tf-idf\n\n","output_type":"stream"},{"execution_count":177,"output_type":"execute_result","data":{"text/plain":"[{'gt', 'lt', 'name'},\n {'com', 'company', 'repo'},\n {'print', 'support', 'version'},\n {'address', 'class', 'model', 'person', 'private'},\n {'cartoon', 'filtereffect', 'look', 'openmovieeditor', 'output'},\n {'gt', 'lt', 'script', 'type', 'value'},\n {'https', 'mozilla', 'mozillalab', 'vcsintegration'},\n {'block', 'bzip2recover', 'eof', 'limit'},\n {'frame', 'id', 'search', 'tag'},\n {'directory', 'p1', 'subdirectory', 'unzip'},\n {'backup', 'http', 'imaging', 'question'},\n {'2322', 'algorithm', 'apriori', 'code', 'transaction'},\n {'classeb', 'code', 'gt', 'lt', 'public'},\n {'completely', 'file', 'finish'},\n {'2005', 'datatype', 'phone', 'sql', 'store'},\n {'artwork', 'colcount', 'mysql', 'result', 'row'},\n {'success', 'trace', 'url', 'urlxml'},\n {'category', 'establish', 'etc', 'side'},\n {'code', 'element', 'em', 'value'},\n {'gms', 'import'}]"},"metadata":{}}]},{"cell_type":"code","source":"false_negative(tags_tfidf, gold_tags, model='tf-idf')","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:20:03.044171Z","iopub.execute_input":"2022-11-19T09:20:03.044449Z","iopub.status.idle":"2022-11-19T09:20:03.054557Z","shell.execute_reply.started":"2022-11-19T09:20:03.044424Z","shell.execute_reply":"2022-11-19T09:20:03.053357Z"},"trusted":true},"execution_count":178,"outputs":[{"name":"stdout","text":"Keywords ошибочно НЕ предложенные моделью tf-idf\n\n","output_type":"stream"},{"execution_count":178,"output_type":"execute_result","data":{"text/plain":"[set(),\n {'clone', 'password', 'server'},\n {'c++'},\n {'dependency', 'hibernate'},\n {'ffmpeg', 'frie0r'},\n {'ajax', 'fwrite', 'javascript', 'jquery', 'php'},\n {'github'},\n {'EOF'},\n {'java', 'javadocs'},\n set(),\n {'ghost'},\n {'R', 'r', 'transactions'},\n {'hdb2ddl', 'hibernate', 'java'},\n {'mysql', 'php'},\n {'bigint', 'sql-server-2005', 'varchar'},\n {'counter', 'php'},\n {'flash'},\n {'search', 'sql'},\n {'arrays', 'c'},\n {'API'}]"},"metadata":{}}]},{"cell_type":"code","source":"false_positive(tags_bm25, gold_tags, model='bm25')","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:20:03.059773Z","iopub.execute_input":"2022-11-19T09:20:03.060894Z","iopub.status.idle":"2022-11-19T09:20:03.071367Z","shell.execute_reply.started":"2022-11-19T09:20:03.060851Z","shell.execute_reply":"2022-11-19T09:20:03.070065Z"},"trusted":true},"execution_count":179,"outputs":[{"name":"stdout","text":"Keywords ошибочно предложенные моделью bm25\n\n","output_type":"stream"},{"execution_count":179,"output_type":"execute_result","data":{"text/plain":"[{'appender', 'jdbc', 'param'},\n {'company', 'repo', 'username'},\n {'print', 'support', 'version'},\n {'address', 'model', 'person', 'private'},\n {'cartoon', 'filtereffect', 'openmovieeditor', 'video'},\n {'gt', 'script', 'text', 'type', 'value'},\n {'https', 'mozilla', 'mozillalab', 'vcsintegration'},\n {'block', 'bzip2recover', 'eof', 'limit'},\n {'frame', 'id', 'search', 'tag'},\n {'7z', 'directory', 'ignore', 'subdirectory'},\n {'2010', 'backup', 'imaging', 'installation'},\n {'1141', '2322', 'algorithm', 'apriori', 'transaction'},\n {'annotazione', 'cb', 'classeb', 'integer', 'numero'},\n {'completely', 'finish', 'run'},\n {'2005', 'datatype', 'phone', 'sql', 'store'},\n {'artwork', 'cell', 'colcount', 'mysql', 'result'},\n {'success', 'trace', 'url', 'urlxml'},\n {'category', 'define', 'establish', 'side'},\n {'1024', 'em', 'iteration', 'value'},\n {'gms', 'import', 'loc'}]"},"metadata":{}}]},{"cell_type":"code","source":"false_negative(tags_bm25, gold_tags, model='bm25')","metadata":{"execution":{"iopub.status.busy":"2022-11-19T09:20:03.072739Z","iopub.execute_input":"2022-11-19T09:20:03.073780Z","iopub.status.idle":"2022-11-19T09:20:03.083781Z","shell.execute_reply.started":"2022-11-19T09:20:03.073742Z","shell.execute_reply":"2022-11-19T09:20:03.082566Z"},"trusted":true},"execution_count":180,"outputs":[{"name":"stdout","text":"Keywords ошибочно НЕ предложенные моделью bm25\n\n","output_type":"stream"},{"execution_count":180,"output_type":"execute_result","data":{"text/plain":"[set(),\n {'clone', 'password', 'server'},\n {'c++'},\n {'hibernate'},\n {'frie0r'},\n {'ajax', 'fwrite', 'javascript', 'jquery', 'php'},\n {'github'},\n {'EOF'},\n {'java', 'javadocs'},\n set(),\n {'ghost'},\n {'R', 'r', 'transactions'},\n {'hdb2ddl', 'hibernate', 'java'},\n {'mysql', 'php'},\n {'bigint', 'sql-server-2005', 'varchar'},\n {'counter', 'php'},\n {'xml'},\n {'search', 'sql'},\n {'arrays', 'c'},\n {'API', 'map'}]"},"metadata":{}}]}]}
